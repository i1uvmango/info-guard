"""
í•™ìŠµëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

import torch
import time
import logging
from models.multitask_credibility_model import MultiTaskCredibilityModel

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def test_model_loading():
    """ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸"""
    try:
        logger.info("ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        model = MultiTaskCredibilityModel()
        logger.info("ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì™„ë£Œ")
        
        # í•™ìŠµëœ ê°€ì¤‘ì¹˜ ë¡œë“œ
        model.load_state_dict(
            torch.load('multitask_credibility_model_optimized.pth', 
                      map_location=model.device)
        )
        logger.info("í•™ìŠµëœ ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ")
        
        # í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
        model.eval()
        logger.info("ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •")
        
        return model
        
    except Exception as e:
        logger.error(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        raise

def test_model_prediction(model):
    """ëª¨ë¸ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸"""
    try:
        logger.info("ëª¨ë¸ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        # í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸ë“¤
        test_texts = [
            "ì´ ë‰´ìŠ¤ëŠ” ì‚¬ì‹¤ì„ ë°”íƒ•ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.",  # ë†’ì€ ì‹ ë¢°ë„
            "ì¶©ê²©ì ì¸ ì§„ì‹¤! ë‹¹ì‹ ì´ ëª°ëë˜ ë†€ë¼ìš´ ì‚¬ì‹¤!",  # ë‚®ì€ ì‹ ë¢°ë„
            "ê³¼í•™ì  ì—°êµ¬ ê²°ê³¼ì— ë”°ë¥´ë©´ ì´ ë°©ë²•ì´ íš¨ê³¼ì ì…ë‹ˆë‹¤.",  # ì¤‘ê°„ ì‹ ë¢°ë„
            "ë¯¿ì„ ìˆ˜ ì—†ëŠ” ì´ì•¼ê¸°! ë°˜ë“œì‹œ ë´ì•¼ í•©ë‹ˆë‹¤!",  # ë‚®ì€ ì‹ ë¢°ë„
            "ì „ë¬¸ê°€ë“¤ì˜ í•©ì˜ì— ë”°ë¥´ë©´ ì´ ì •ì±…ì´ íš¨ê³¼ì ì¼ ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤."  # ë†’ì€ ì‹ ë¢°ë„
        ]
        
        results = []
        
        for i, text in enumerate(test_texts):
            logger.info(f"í…ìŠ¤íŠ¸ {i+1} ë¶„ì„ ì¤‘: {text[:50]}...")
            
            start_time = time.time()
            
            # ë‹¨ì¼ íƒœìŠ¤í¬ ì˜ˆì¸¡
            single_result = model.predict_credibility(text, "harmlessness")
            
            # ë©€í‹°íƒœìŠ¤í¬ ì˜ˆì¸¡
            multi_result = model.predict_multiple_tasks(text)
            
            processing_time = time.time() - start_time
            
            result = {
                'text': text,
                'single_task': single_result,
                'multi_task': multi_result,
                'processing_time': processing_time
            }
            
            results.append(result)
            
            logger.info(f"í…ìŠ¤íŠ¸ {i+1} ë¶„ì„ ì™„ë£Œ - ì²˜ë¦¬ì‹œê°„: {processing_time:.2f}ì´ˆ")
            logger.info(f"  ë‹¨ì¼ íƒœìŠ¤í¬ ì ìˆ˜: {single_result['safety_score']:.1f}")
            logger.info(f"  ë©€í‹°íƒœìŠ¤í¬ ì ìˆ˜: {multi_result['overall']['credibility_score']:.1f}")
        
        return results
        
    except Exception as e:
        logger.error(f"ëª¨ë¸ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        raise

def test_gpu_memory_usage():
    """GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸"""
    try:
        if torch.cuda.is_available():
            logger.info("GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸...")
            
            # ì´ˆê¸° ë©”ëª¨ë¦¬ ìƒíƒœ
            torch.cuda.empty_cache()
            initial_memory = torch.cuda.memory_allocated() / 1024**2
            logger.info(f"ì´ˆê¸° GPU ë©”ëª¨ë¦¬: {initial_memory:.1f}MB")
            
            # ëª¨ë¸ ë¡œë“œ í›„ ë©”ëª¨ë¦¬
            model = test_model_loading()
            model_memory = torch.cuda.memory_allocated() / 1024**2
            logger.info(f"ëª¨ë¸ ë¡œë“œ í›„ GPU ë©”ëª¨ë¦¬: {model_memory:.1f}MB")
            logger.info(f"ëª¨ë¸ ì‚¬ìš© ë©”ëª¨ë¦¬: {model_memory - initial_memory:.1f}MB")
            
            # ì˜ˆì¸¡ í›„ ë©”ëª¨ë¦¬
            test_model_prediction(model)
            final_memory = torch.cuda.memory_allocated() / 1024**2
            logger.info(f"ì˜ˆì¸¡ í›„ GPU ë©”ëª¨ë¦¬: {final_memory:.1f}MB")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del model
            torch.cuda.empty_cache()
            cleaned_memory = torch.cuda.memory_allocated() / 1024**2
            logger.info(f"ì •ë¦¬ í›„ GPU ë©”ëª¨ë¦¬: {cleaned_memory:.1f}MB")
            
        else:
            logger.info("GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.")
            
    except Exception as e:
        logger.error(f"GPU ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

def test_input_output_format():
    """ì…ë ¥/ì¶œë ¥ í˜•ì‹ í…ŒìŠ¤íŠ¸"""
    try:
        logger.info("ì…ë ¥/ì¶œë ¥ í˜•ì‹ í…ŒìŠ¤íŠ¸...")
        
        model = test_model_loading()
        
        # í…ŒìŠ¤íŠ¸ ì…ë ¥
        test_input = {
            "text": "ì´ê²ƒì€ í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.",
            "title": "í…ŒìŠ¤íŠ¸ ì œëª©",
            "description": "í…ŒìŠ¤íŠ¸ ì„¤ëª…",
            "transcript": "í…ŒìŠ¤íŠ¸ ìë§‰"
        }
        
        # í…ìŠ¤íŠ¸ ê²°í•©
        combined_text = " ".join([
            test_input["title"],
            test_input["description"], 
            test_input["transcript"]
        ])
        
        logger.info(f"ê²°í•©ëœ í…ìŠ¤íŠ¸: {combined_text}")
        
        # ì˜ˆì¸¡ ì‹¤í–‰
        result = model.predict_multiple_tasks(combined_text)
        
        # ì¶œë ¥ í˜•ì‹ í™•ì¸
        expected_keys = ['harmlessness', 'honesty', 'helpfulness', 'overall']
        for key in expected_keys:
            assert key in result, f"ê²°ê³¼ì— {key} í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤"
        
        logger.info("ì¶œë ¥ í˜•ì‹ í…ŒìŠ¤íŠ¸ í†µê³¼")
        logger.info(f"ê²°ê³¼ êµ¬ì¡°: {list(result.keys())}")
        
        return result
        
    except Exception as e:
        logger.error(f"ì…ë ¥/ì¶œë ¥ í˜•ì‹ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        raise

def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    logger.info("í•™ìŠµëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    try:
        # 1. ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸
        logger.info("=== 1. ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸ ===")
        model = test_model_loading()
        logger.info("âœ… ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸ í†µê³¼")
        
        # 2. ì…ë ¥/ì¶œë ¥ í˜•ì‹ í…ŒìŠ¤íŠ¸
        logger.info("=== 2. ì…ë ¥/ì¶œë ¥ í˜•ì‹ í…ŒìŠ¤íŠ¸ ===")
        format_result = test_input_output_format()
        logger.info("âœ… ì…ë ¥/ì¶œë ¥ í˜•ì‹ í…ŒìŠ¤íŠ¸ í†µê³¼")
        
        # 3. ëª¨ë¸ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
        logger.info("=== 3. ëª¨ë¸ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸ ===")
        prediction_results = test_model_prediction(model)
        logger.info("âœ… ëª¨ë¸ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸ í†µê³¼")
        
        # 4. GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸
        logger.info("=== 4. GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸ ===")
        test_gpu_memory_usage()
        logger.info("âœ… GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸ í†µê³¼")
        
        # 5. ê²°ê³¼ ìš”ì•½
        logger.info("=== í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½ ===")
        logger.info(f"ì´ {len(prediction_results)}ê°œ í…ìŠ¤íŠ¸ ë¶„ì„ ì™„ë£Œ")
        
        for i, result in enumerate(prediction_results):
            overall_score = result['multi_task']['overall']['credibility_score']
            processing_time = result['processing_time']
            logger.info(f"í…ìŠ¤íŠ¸ {i+1}: ì ìˆ˜ {overall_score:.1f}, ì²˜ë¦¬ì‹œê°„ {processing_time:.2f}ì´ˆ")
        
        logger.info("ğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!")
        
    except Exception as e:
        logger.error(f"í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        raise

if __name__ == "__main__":
    main() 