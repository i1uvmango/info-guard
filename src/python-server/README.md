# Info-Guard Python AI Server

RTX 4060Ti 16GBμ— μµμ ν™”λ AI κΈ°λ° YouTube μμƒ μ‹ λΆ°μ„± λ¶„μ„ μ„λ²„μ…λ‹λ‹¤.

## π€ μ£Όμ” κΈ°λ¥

- **AI κΈ°λ° μ‹ λΆ°μ„± λ¶„μ„**: κ°μ • λ¶„μ„, νΈν–¥ κ°μ§€, μ‚¬μ‹¤ ν™•μΈ, μ¶μ² κ²€μ¦
- **YouTube API μ—°λ™**: μμƒ μ •λ³΄ λ° μλ§‰ μλ™ μμ§‘
- **μ‹¤μ‹κ°„ λ¶„μ„**: WebSocketμ„ ν†µν• μ‹¤μ‹κ°„ λ¶„μ„ κ²°κ³Ό μ „μ†΅
- **CUDA μµμ ν™”**: RTX 4060Ti 16GBμ— μµμ ν™”λ GPU κ°€μ†
- **λ¨λΈ ν•™μµ**: μ»¤μ¤ν…€ λ°μ΄ν„°μ…‹μ„ ν†µν• λ¨λΈ νμΈνλ‹

## π› οΈ μ‹μ¤ν… μ”κµ¬μ‚¬ν•­

### ν•λ“μ›¨μ–΄
- **GPU**: NVIDIA RTX 4060Ti 16GB μ΄μƒ
- **RAM**: 32GB μ΄μƒ κ¶μ¥
- **μ €μ¥κ³µκ°„**: 50GB μ΄μƒ (λ¨λΈ μΊμ‹ ν¬ν•¨)

### μ†ν”„νΈμ›¨μ–΄
- **OS**: Ubuntu 20.04+ / Windows 11 / macOS 13+
- **Python**: 3.9+ (3.11 κ¶μ¥)
- **CUDA**: 12.1+ (PyTorch νΈν™ λ²„μ „)
- **cuDNN**: 8.9+ (CUDA 12.1 νΈν™)

## π“¦ μ„¤μΉ λ°©λ²•

### 1. ν™κ²½ μ„¤μ •

```bash
# κ°€μƒν™κ²½ μƒμ„±
python -m venv info-guard-env
source info-guard-env/bin/activate  # Linux/macOS
# λλ”
info-guard-env\Scripts\activate     # Windows

# Python λ²„μ „ ν™•μΈ (3.9+ ν•„μ”)
python --version
```

### 2. CUDA μ„¤μΉ ν™•μΈ

```bash
# CUDA λ²„μ „ ν™•μΈ
nvidia-smi
nvcc --version

# PyTorch CUDA μ§€μ› ν™•μΈ
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}'); print(f'CUDA version: {torch.version.cuda}')"
```

### 3. μμ΅΄μ„± μ„¤μΉ

```bash
# κΈ°λ³Έ μμ΅΄μ„± μ„¤μΉ
pip install -r requirements.txt

# λλ” λ‹¨κ³„λ³„ μ„¤μΉ (κ¶μ¥)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
pip install transformers datasets accelerate
pip install fastapi uvicorn
pip install google-api-python-client youtube-transcript-api
pip install scikit-learn numpy pandas
pip install sentence-transformers
```

### 4. ν™κ²½ λ³€μ μ„¤μ •

```bash
# .env νμΌ λ³µμ‚¬
cp env.example .env

# YouTube API ν‚¤ μ„¤μ •
echo "YOUTUBE_API_KEY=AIzaSyC8_h83XbrUYo-jJJGdgHzJbZLoVaKJcd4" >> .env

# CUDA μ„¤μ • (RTX 4060Ti μµμ ν™”)
echo "CUDA_VISIBLE_DEVICES=0" >> .env
echo "MAX_MEMORY_MB=14000" >> .env
echo "MIXED_PRECISION=true" >> .env
echo "GRADIENT_CHECKPOINTING=true" >> .env
```

## π”§ μ„¤μ • μµμ ν™”

### CUDA λ©”λ¨λ¦¬ μµμ ν™”

```python
# utils/config.pyμ—μ„ μ„¤μ • μ΅°μ •
MAX_MEMORY_MB = 14000        # 16GB - 2GB μ—¬μ 
TRAINING_BATCH_SIZE = 4      # RTX 4060Tiμ— μµμ ν™”
INFERENCE_BATCH_SIZE = 8     # μ¶”λ΅  μ‹ λ” ν° λ°°μΉ
GRADIENT_ACCUMULATION_STEPS = 4  # λ©”λ¨λ¦¬ μ μ•½
```

### λ¨λΈ λ΅λ”© μ „λµ

```python
# μλ™ λ””λ°”μ΄μ¤ λ§¤ν•‘ (κ¶μ¥)
DEVICE_MAP = "auto"
LOW_CPU_MEM_USAGE = True

# λλ” μλ™ μ„¤μ •
DEVICE_MAP = None  # μλ™μΌλ΅ GPUμ— λ΅λ“
```

## π€ μ‹¤ν–‰ λ°©λ²•

### 1. κ°λ° λ¨λ“ μ‹¤ν–‰

```bash
# μ„λ²„ μ‹¤ν–‰
python main.py

# λλ” uvicorn μ§μ ‘ μ‹¤ν–‰
uvicorn main:app --host 0.0.0.0 --port 8000 --reload
```

### 2. ν”„λ΅λ•μ… λ¨λ“ μ‹¤ν–‰

```bash
# Gunicorn μ‚¬μ© (Linux/macOS)
pip install gunicorn
gunicorn main:app -w 4 -k uvicorn.workers.UvicornWorker --bind 0.0.0.0:8000

# Windows
waitress-serve main:app --host=0.0.0.0 --port=8000
```

### 3. Docker μ‹¤ν–‰

```bash
# Docker Compose μ‚¬μ©
cd ../docker
docker-compose up python-server

# λλ” κ°λ³„ μ‹¤ν–‰
docker build -t info-guard-python .
docker run -p 8000:8000 --gpus all info-guard-python
```

## π§  AI λ¨λΈ ν•™μµ

### 1. λ°μ΄ν„° μ¤€λΉ„

```bash
# ν•™μµ λ°μ΄ν„° ν•μ‹
{
    "text": "λ¶„μ„ν•  ν…μ¤νΈ",
    "label": "positive|neutral|negative"  # κ°μ • λ¶„μ„
    "bias_label": "biased|neutral"        # νΈν–¥ κ°μ§€
}
```

### 2. λ¨λΈ ν•™μµ μ‹¤ν–‰

```bash
# κ°μ • λ¶„μ„ λ¨λΈ ν•™μµ
python scripts/train_models.py --model sentiment --data-path ./data/sentiment_data.json

# νΈν–¥ κ°μ§€ λ¨λΈ ν•™μµ
python scripts/train_models.py --model bias --data-path ./data/bias_data.json

# λ¨λ“  λ¨λΈ ν•™μµ
python scripts/train_models.py --model all --data-path ./data/
```

### 3. ν•™μµ λ¨λ‹ν„°λ§

```bash
# TensorBoard μ‹¤ν–‰
tensorboard --logdir ./training_outputs

# λ©”λ¨λ¦¬ μ‚¬μ©λ‰ ν™•μΈ
python -c "from ai_models.model_loader import model_loader; print(model_loader.get_memory_usage())"
```

## π“ API μ—”λ“ν¬μΈνΈ

### κΈ°λ³Έ μ—”λ“ν¬μΈνΈ

- `GET /health`: μ„λ²„ μƒνƒ ν™•μΈ
- `POST /analysis`: μμƒ μ‹ λΆ°μ„± λ¶„μ„
- `GET /analysis/{video_id}`: λ¶„μ„ κ²°κ³Ό μ΅°ν
- `WebSocket /ws`: μ‹¤μ‹κ°„ λ¶„μ„ μ§„ν–‰ μƒν™©

### λ¶„μ„ μ”μ²­ μμ‹

```python
import requests

# λ¶„μ„ μ”μ²­
response = requests.post("http://localhost:8000/analysis", json={
    "video_url": "https://www.youtube.com/watch?v=VIDEO_ID",
    "analysis_type": "full",  # full, quick, custom
    "include_transcript": True,
    "include_comments": False
})

print(response.json())
```

## π” λ¬Έμ  ν•΄κ²°

### CUDA λ©”λ¨λ¦¬ λ¶€μ΅±

```bash
# λ°°μΉ ν¬κΈ° μ¤„μ΄κΈ°
echo "TRAINING_BATCH_SIZE=2" >> .env
echo "INFERENCE_BATCH_SIZE=4" >> .env

# κ·Έλλ””μ–ΈνΈ μ²΄ν¬ν¬μΈν… ν™μ„±ν™”
echo "GRADIENT_CHECKPOINTING=true" >> .env

# λ©”λ¨λ¦¬ μ •λ¦¬
python -c "import torch; torch.cuda.empty_cache()"
```

### λ¨λΈ λ΅λ”© μ‹¤ν¨

```bash
# μΊμ‹ μ •λ¦¬
rm -rf ./ai_models/cache/*

# λ¨λΈ μ¬λ‹¤μ΄λ΅λ“
python -c "from ai_models.model_loader import model_loader; model_loader.cleanup()"
```

### YouTube API μ ν•

```bash
# API μ ν• μ„¤μ • μ΅°μ •
echo "MAX_REQUESTS_PER_MINUTE=50" >> .env
echo "MAX_CONCURRENT_REQUESTS=5" >> .env
```

## π“ μ„±λ¥ μµμ ν™”

### RTX 4060Ti νΉν™” μ„¤μ •

```python
# utils/config.py
TORCH_CUDA_ARCH_LIST = "8.9"  # RTX 4060Ti μ•„ν‚¤ν…μ²
MAX_MEMORY_MB = 14000         # 16GB μµμ ν™”
MIXED_PRECISION = True         # FP16 μ‚¬μ©
GRADIENT_CHECKPOINTING = True  # λ©”λ¨λ¦¬ μ μ•½
```

### λ°°μΉ μ²λ¦¬ μµμ ν™”

```python
# ν•™μµ μ‹
TRAINING_BATCH_SIZE = 4
GRADIENT_ACCUMULATION_STEPS = 4

# μ¶”λ΅  μ‹
INFERENCE_BATCH_SIZE = 8
```

## π§ ν…μ¤νΈ

### λ‹¨μ„ ν…μ¤νΈ

```bash
# λ¨λ“  ν…μ¤νΈ μ‹¤ν–‰
pytest

# νΉμ • λ¨λ“ ν…μ¤νΈ
pytest tests/test_credibility_analyzer.py

# μ»¤λ²„λ¦¬μ§€ ν¬ν•¨
pytest --cov=src/python_server
```

### ν†µν•© ν…μ¤νΈ

```bash
# API ν…μ¤νΈ
python -m pytest tests/test_api.py

# λ¨λΈ ν…μ¤νΈ
python -m pytest tests/test_models.py
```

## π“ μ¶”κ°€ λ¦¬μ†μ¤

- [PyTorch CUDA κ°€μ΄λ“](https://pytorch.org/docs/stable/notes/cuda.html)
- [Transformers μµμ ν™”](https://huggingface.co/docs/transformers/performance)
- [RTX 4060Ti μ„±λ¥ κ°€μ΄λ“](https://www.nvidia.com/en-us/geforce/graphics-cards/rtx-4060-ti/)

## π¤ κΈ°μ—¬ν•κΈ°

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

## π“„ λΌμ΄μ„ μ¤

μ΄ ν”„λ΅μ νΈλ” MIT λΌμ΄μ„ μ¤ ν•μ— λ°°ν¬λ©λ‹λ‹¤.

## π“ μ§€μ›

λ¬Έμ κ°€ λ°μƒν•κ±°λ‚ μ§λ¬Έμ΄ μμΌμ‹λ©΄ μ΄μλ¥Ό μƒμ„±ν•΄μ£Όμ„Έμ”.
