"""
GPU ê°€ì† ì„¤ì • ë° ëª¨ë¸ ìµœì í™” ì„¤ì •
RTX 4060Ti 16GB ìµœì í™”ë¥¼ ìœ„í•œ ì„¤ì •
"""

import os
import torch
from typing import Dict, Any, Optional
from loguru import logger


class GPUConfig:
    """GPU ì„¤ì • ë° ìµœì í™” ê´€ë¦¬"""
    
    def __init__(self):
        self.device = self._get_optimal_device()
        self.gpu_memory_limit = self._get_gpu_memory_limit()
        self.batch_size = self._get_optimal_batch_size()
        self.model_precision = self._get_optimal_precision()
        
    def _get_optimal_device(self) -> str:
        """ìµœì ì˜ ë””ë°”ì´ìŠ¤ ì„ íƒ"""
        if torch.cuda.is_available():
            # CUDA ë””ë°”ì´ìŠ¤ ì •ë³´ í™•ì¸
            gpu_count = torch.cuda.device_count()
            logger.info(f"ğŸ–¥ï¸  ì‚¬ìš© ê°€ëŠ¥í•œ GPU: {gpu_count}ê°œ")
            
            for i in range(gpu_count):
                gpu_name = torch.cuda.get_device_name(i)
                gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
                logger.info(f"  GPU {i}: {gpu_name} ({gpu_memory:.1f}GB)")
            
            # RTX 4060Ti ìš°ì„  ì„ íƒ (16GB VRAM)
            for i in range(gpu_count):
                gpu_name = torch.cuda.get_device_name(i)
                if "4060" in gpu_name or "16GB" in gpu_name:
                    torch.cuda.set_device(i)
                    logger.info(f"âœ… RTX 4060Ti ì„ íƒë¨: GPU {i}")
                    return f"cuda:{i}"
            
            # ì²« ë²ˆì§¸ GPU ì‚¬ìš©
            torch.cuda.set_device(0)
            logger.info(f"âœ… GPU 0 ì„ íƒë¨")
            return "cuda:0"
        else:
            logger.warning("âš ï¸  CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
            return "cpu"
    
    def _get_gpu_memory_limit(self) -> int:
        """GPU ë©”ëª¨ë¦¬ ì œí•œ ì„¤ì • (MB)"""
        if self.device.startswith("cuda"):
            # RTX 4060Ti 16GB ì¤‘ 14GB ì‚¬ìš© (2GB ì—¬ìœ )
            return 14 * 1024
        return 0
    
    def _get_optimal_batch_size(self) -> int:
        """ìµœì  ë°°ì¹˜ í¬ê¸° ê³„ì‚°"""
        if self.device.startswith("cuda"):
            # GPU ë©”ëª¨ë¦¬ì— ë”°ë¥¸ ë°°ì¹˜ í¬ê¸° ì¡°ì •
            if self.gpu_memory_limit >= 14 * 1024:  # 14GB ì´ìƒ
                return 8
            elif self.gpu_memory_limit >= 8 * 1024:  # 8GB ì´ìƒ
                return 4
            else:
                return 2
        return 1
    
    def _get_optimal_precision(self) -> str:
        """ìµœì  ì •ë°€ë„ ì„ íƒ"""
        if self.device.startswith("cuda"):
            # RTX 4060TiëŠ” FP16 ì§€ì›
            return "fp16"
        return "fp32"
    
    def get_torch_config(self) -> Dict[str, Any]:
        """PyTorch ì„¤ì • ë°˜í™˜"""
        config = {
            "device": self.device,
            "dtype": torch.float16 if self.model_precision == "fp16" else torch.float32,
            "batch_size": self.batch_size,
            "precision": self.model_precision
        }
        
        if self.device.startswith("cuda"):
            config.update({
                "cuda_available": True,
                "gpu_memory_limit": self.gpu_memory_limit,
                "gpu_name": torch.cuda.get_device_name(),
                "gpu_memory_allocated": torch.cuda.memory_allocated() / 1024**2,
                "gpu_memory_reserved": torch.cuda.memory_reserved() / 1024**2
            })
        
        return config
    
    def optimize_memory(self):
        """GPU ë©”ëª¨ë¦¬ ìµœì í™”"""
        if self.device.startswith("cuda"):
            # ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬
            torch.cuda.empty_cache()
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
            allocated = torch.cuda.memory_allocated() / 1024**2
            reserved = torch.cuda.memory_reserved() / 1024**2
            
            logger.info(f"ğŸ’¾ GPU ë©”ëª¨ë¦¬ ìƒíƒœ: í• ë‹¹ë¨ {allocated:.1f}MB, ì˜ˆì•½ë¨ {reserved:.1f}MB")
    
    def get_model_loading_config(self) -> Dict[str, Any]:
        """ëª¨ë¸ ë¡œë”© ìµœì í™” ì„¤ì •"""
        return {
            "device_map": "auto" if self.device.startswith("cuda") else None,
            "torch_dtype": torch.float16 if self.model_precision == "fp16" else torch.float32,
            "low_cpu_mem_usage": True,
            "offload_folder": "offload" if self.device.startswith("cuda") else None
        }


# ì „ì—­ GPU ì„¤ì • ì¸ìŠ¤í„´ìŠ¤
gpu_config = GPUConfig()


def get_gpu_config() -> GPUConfig:
    """GPU ì„¤ì • ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    return gpu_config


def is_gpu_available() -> bool:
    """GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
    return gpu_config.device.startswith("cuda")


def get_optimal_device() -> str:
    """ìµœì  ë””ë°”ì´ìŠ¤ ë°˜í™˜"""
    return gpu_config.device
