"""
AI ëª¨ë¸ ê¸°ë³¸ í´ë˜ìŠ¤
ëª¨ë“  AI ëª¨ë¸ì´ ìƒì†ë°›ì•„ì•¼ í•˜ëŠ” ê¸°ë³¸ í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
"""

import os
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional, Union
from pathlib import Path
import torch
from transformers import AutoTokenizer, AutoModel
from loguru import logger

from app.core.gpu_config import get_gpu_config, is_gpu_available


class BaseAIModel(ABC):
    """AI ëª¨ë¸ì˜ ê¸°ë³¸ í´ë˜ìŠ¤"""
    
    def __init__(self, model_name: str, device: str = "cpu"):
        self.model_name = model_name
        self.is_loaded = False
        self.model = None
        self.tokenizer = None
        self.gpu_config = get_gpu_config()
        self.device = device
        
    @abstractmethod
    def analyze(self, text: str, **kwargs) -> Any:
        """í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
        pass
    
    @abstractmethod
    def load_model(self) -> bool:
        """ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤. í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤."""
        pass
    
    def load_huggingface_model(self, model_name: str, task: str) -> bool:
        """Hugging Face ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            logger.info(f"Hugging Face ëª¨ë¸ ë¡œë”© ì‹œì‘: {model_name}")
            
            # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
            self.model = AutoModel.from_pretrained(model_name)
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            
            # GPU ì‚¬ìš© ê°€ëŠ¥ì‹œ GPUë¡œ ì´ë™
            if self.device.startswith("cuda") and torch.cuda.is_available():
                self.model = self.model.to(self.device)
                logger.info(f"ëª¨ë¸ì„ GPUë¡œ ì´ë™: {self.device}")
            else:
                self.device = "cpu"
                logger.info("ëª¨ë¸ì„ CPUë¡œ ì´ë™")
            
            self.is_loaded = True
            logger.info(f"âœ… Hugging Face ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {model_name}")
            return True
            
        except Exception as e:
            logger.error(f"Hugging Face ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            self.is_loaded = False
            return False
    
    def unload_model(self) -> bool:
        """ëª¨ë¸ì„ ì–¸ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            if not self.is_loaded:
                return True
            
            # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì •ë¦¬
            if hasattr(self, 'model'):
                del self.model
                self.model = None
            
            if hasattr(self, 'tokenizer'):
                del self.tokenizer
                self.tokenizer = None
            
            self.is_loaded = False
            logger.info("ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.error(f"ëª¨ë¸ ì–¸ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def ensure_model_loaded(self) -> bool:
        """ëª¨ë¸ì´ ë¡œë“œë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³ , í•„ìš”ì‹œ ë¡œë“œí•©ë‹ˆë‹¤."""
        if not self.is_loaded:
            logger.info(f"ğŸ”„ {self.model_name} ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¡œë”©ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
            return self.load_model()
        return True
    
    def get_status(self) -> Dict[str, Any]:
        """ëª¨ë¸ ìƒíƒœë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        status = {
            "model_name": self.model_name,
            "is_loaded": self.is_loaded,
            "device": self.device,
            "gpu_available": is_gpu_available()
        }
        
        if is_gpu_available():
            status.update({
                "gpu_memory_allocated": torch.cuda.memory_allocated() / 1024**2,
                "gpu_memory_reserved": torch.cuda.memory_reserved() / 1024**2
            })
        
        return status
    
    def get_model_info(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        if not self.is_loaded:
            return {"error": "ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
        
        info = {
            "model_name": self.model_name,
            "model_type": type(self.model).__name__,
            "tokenizer_type": type(self.tokenizer).__name__,
            "device": str(next(self.model.parameters()).device) if self.model else None,
            "parameters": sum(p.numel() for p in self.model.parameters()) if self.model else 0
        }
        
        return info
    
    def health_check(self) -> Dict[str, Any]:
        """ëª¨ë¸ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤."""
        try:
            status = {
                "model_name": self.model_name,
                "is_loaded": self.is_loaded,
                "device": self.device,
                "gpu_available": torch.cuda.is_available() if self.device.startswith("cuda") else False
            }
            
            if self.is_loaded:
                status.update({
                    "model_type": type(self.model).__name__ if self.model else None,
                    "tokenizer_type": type(self.tokenizer).__name__ if self.tokenizer else None
                })
            
            return status
            
        except Exception as e:
            logger.error(f"í—¬ìŠ¤ ì²´í¬ ì‹¤íŒ¨: {e}")
            return {
                "model_name": self.model_name,
                "error": str(e)
            }
    
    def __del__(self):
        """ì†Œë©¸ì: GPU ë©”ëª¨ë¦¬ ì •ë¦¬"""
        try:
            if hasattr(self, 'model') and self.model is not None:
                if is_gpu_available():
                    del self.model
                    torch.cuda.empty_cache()
        except:
            pass
